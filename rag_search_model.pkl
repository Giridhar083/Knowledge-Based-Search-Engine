# -*- coding: utf-8 -*-
"""KBSE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RPYraawZYB839Kw_9b5FEVngHARGVy-x
"""

!pip install langchain-community

!pip install chromadb

!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \
bitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12

!pip install tokenizers

!pip install bitsandbytes

!pip install -U bitsandbytes

from torch import cuda, bfloat16
import torch
import transformers
from time import time
import chromadb
from chromadb.config import Settings
from langchain.llms import HuggingFacePipeline
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma

from google.colab import userdata
from huggingface_hub import login
hf_token = userdata.get('Rag')
login(token=hf_token)
model_id = "meta-llama/Llama-2-7b-chat-hf"
device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'
bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)

from transformers import AutoTokenizer

time_1 = time()
model_config = transformers.AutoConfig.from_pretrained(
    model_id,
)
model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    config=model_config,
    quantization_config=bnb_config,
    device_map='auto',
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
time_2 = time()
print(f"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.")

time_1 = time()
query_pipeline = transformers.pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.float16,
        device_map="auto",)
time_2 = time()
print(f"Prepare pipeline: {round(time_2-time_1, 3)} sec.")

def test_model(tokenizer, pipeline, prompt_to_test):
    """
    Perform a query
    print the result
    Args:
        tokenizer: the tokenizer
        pipeline: the pipeline
        prompt_to_test: the prompt
    Returns
        None
    """
    time_1 = time()
    sequences = pipeline(
        prompt_to_test,
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        max_length=200,)
    time_2 = time()
    print(f"Test inference: {round(time_2-time_1, 3)} sec.")
    for seq in sequences:
        print(f"Result: {seq['generated_text']}")

test_model(tokenizer,query_pipeline,"Using these documents, answer the user’s question succinctly. Keep it in 100 words.")

"""# RAG Building"""

llm = HuggingFacePipeline(pipeline=query_pipeline)
llm(prompt="Using these documents, answer the user’s question succinctly. Keep it in 100 words.")

from google.colab import files
uploaded = files.upload()

import os
os.listdir("/content")

from langchain.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader
txt_loader = DirectoryLoader("/content", glob="*.txt", loader_cls=TextLoader)
pdf_loader = DirectoryLoader("/content", glob="*.pdf", loader_cls=PyPDFLoader)


docs = txt_loader.load() + pdf_loader.load()
print("Total documents loaded:", len(docs))

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)
all_splits = text_splitter.split_documents(docs)

model_name = "sentence-transformers/all-mpnet-base-v2"
model_kwargs = {"device": "cuda"}

embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)

vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory="chroma_db")

retriever = vectordb.as_retriever()

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    verbose=True
)

def test_rag(qa, query):
    print(f"Query: {query}\n")
    time_1 = time()
    result = qa.run(query)
    time_2 = time()
    print(f"Inference time: {round(time_2-time_1, 3)} sec.")
    print("\nResult: ", result)

import pickle
with open('rag_search_model.pkl', 'wb') as f:
    pickle.dump(qa, f)

# To reconstruct the qa object, you first need to load the vector database
# Make sure you have already loaded your language model ('llm') as done before

# Load the persisted Chroma database
loaded_vectordb = Chroma(persist_directory="chroma_db", embedding_function=embeddings)

# Create a new retriever from the loaded vector database
loaded_retriever = loaded_vectordb.as_retriever()

# Reconstruct the RetrievalQA chain
reconstructed_qa = RetrievalQA.from_chain_type(
    llm=llm,  # Use the previously loaded language model
    chain_type="stuff",
    retriever=loaded_retriever,
    verbose=True
)

print("RAG object reconstructed successfully!")
# You can now use 'reconstructed_qa' for your queries
# test_rag(reconstructed_qa, "Your query here")

query = "What were the main topics in the document? Summarize. Keep it under 50 words."
test_rag(qa, query)

query = "What is the main topic of the document?"
answer = reconstructed_qa.run(query)
print(answer)

query = "Summarize. Keep it under 100 words."
test_rag(qa, query)

docs = vectordb.similarity_search(query)
print(f"Query: {query}")
print(f"Retrieved documents: {len(docs)}")
for doc in docs:
    doc_details = doc.to_json()['kwargs']
    print("Source: ", doc_details['metadata']['source'])
    print("Text: ", doc_details['page_content'], "\n")

query = "Is there any mission where nasa and isro are collabrating together. Keep it under 100 words."
test_rag(qa, query)

